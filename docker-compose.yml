services:
  backend:
    image: ${BACKEND_IMAGE:-llm-council-backend}:${IMAGE_TAG:-latest}
    build:
      context: .
      dockerfile: Dockerfile
      target: backend
      args:
        OCI_IMAGE_SOURCE: ${OCI_IMAGE_SOURCE:-https://github.com/ieaves/llm-council}
    env_file:
      - .env
    environment:
      DATA_DIR: /app/data/conversations
      OLLAMA_API_URL: http://host.docker.internal:8080 # Only needed if you want to use Ollama
      RAMALAMA_STORE: ${MODEL_CACHE} # If you want to run local models without Ollama
    security_opt:
      - label=disable
    volumes:
      - data:/app/data/conversations
      - /var/run/docker.sock:/var/run/docker.sock  # If you use docker and want to run local models without Ollama
      #- /run/podman/podman.sock:/var/run/docker.sock # If you use podman and want to run local models without Ollama
      - ${MODEL_CACHE}:${MODEL_CACHE}  # If you want to run local models without Ollama
    ports:
      - "8001:8001"

  frontend:
    image: ${FRONTEND_IMAGE:-llm-council-frontend}:${IMAGE_TAG:-latest}
    build:
      context: ./frontend
      dockerfile: Dockerfile
      target: runtime
      args:
        VITE_API_BASE: ${VITE_API_BASE:-http://localhost:8001}
        OCI_IMAGE_SOURCE: ${OCI_IMAGE_SOURCE:-https://github.com/ieaves/llm-council}
    depends_on:
      - backend
    ports:
      - "5173:80"

volumes:
  data:
